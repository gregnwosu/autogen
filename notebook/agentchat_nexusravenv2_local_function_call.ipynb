{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae1f50ec",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/agentchat_function_call.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a71fa36",
   "metadata": {},
   "source": [
    "# Auto Generated Agent Chat: Task Solving with Provided Tools as Functions\n",
    "\n",
    "AutoGen offers conversable agents powered by LLM, tool, or human, which can be used to perform tasks collectively via automated chat. This framework allows tool use and human participation through multi-agent conversation. Please find documentation about this feature [here](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat).\n",
    "\n",
    "In this notebook, we demonstrate how to use `NexusFunctionCallingAssistant` and `UserProxyAgent` to make function calls with the new feature of OpenAI models (in model version 0613). A specified prompt and function configs must be passed to `NexusFunctionCallingAssistant` to initialize the agent. The corresponding functions must be passed to `UserProxyAgent`, which will execute any function calls made by `NexusFunctionCallingAssistant`. Besides this requirement of matching descriptions with functions, we recommend checking the system message in the `NexusFunctionCallingAssistant` to ensure the instructions align with the function call descriptions.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "AutoGen requires `Python>=3.8`. To run this notebook example, please install `pyautogen`:\n",
    "```bash\n",
    "pip install pyautogen\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b803c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install \"pyautogen>=0.2.3\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Required Libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d698366666910d4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Literal\n",
    "from typing_extensions import Annotated\n",
    "import autogen\n",
    "import autogen.agentchat.contrib.nexusravenv2_local_function_calling as Nexus"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e8cce9864107b81"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install Ollama"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1bcc7bbd1ede287"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if os.name == \"posix\":  # Linux or macOS\n",
    "    !curl -fsSL https://ollama.com/install.sh | sh\n",
    "elif os.name == \"nt\":  # Windows\n",
    "    !winget install Ollama.Ollama"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "528c7bd2795ea910"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run Required Library\n",
    "NexusRaven is a standard [Ollama Library](https://ollama.com/library/nexusraven)\n",
    "Ollama binds 127.0.0.1 port 11434 by default.\n",
    "Change the bind address with the OLLAMA_HOST environment variable. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c901b495ead4fdd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%ollama run nexusraven"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "774e76cac317e340"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ebd2397",
   "metadata": {},
   "source": [
    "## Set your API Endpoint\n",
    "\n",
    "Check the ollama console for the network address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "network_address = \"127.0.0.1:11434\"  # this may change please check the output above\n",
    "\n",
    "llm_config = {\n",
    "    \"config_list\": [{\"model\": \"litellmnotneeded\", \"api_key\": \"NotRequired\", \"base_url\": f\"http://{network_address}\"}],\n",
    "    \"cache_seed\": None,\n",
    "}  ## CRITICAL - ENSURE THERE'S NO CACHING FOR TESTING"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f365b00569568eeb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "chatbot = Nexus.NexusFunctionCallingAssistant(\n",
    "    name=\"chatbot\",\n",
    "    # Iteration 4, two examples to help guide LLM on response\n",
    "    system_message=\"\"\"For currency exchange tasks, \n",
    "        only use the functions you have been provided with. \n",
    "        Output 'BAZINGA!' when an answer has been provided. \n",
    "        Do not include the function name or result in the JSON.\n",
    "        Example of the return JSON is: \n",
    "        {\n",
    "            \"parameter_1_name\": 100.00,\n",
    "            \"parameter_2_name\": \"ABC\",\n",
    "            \"parameter_3_name\": \"DEF\",\n",
    "        }. \n",
    "        Another example of the return JSON is: \n",
    "        {\n",
    "            \"parameter_1_name\": \"GHI\",\n",
    "            \"parameter_2_name\": \"ABC\",\n",
    "            \"parameter_3_name\": \"DEF\",\n",
    "            \"parameter_4_name\": 123.00,\n",
    "        }. \"\"\",  # MS - this was needed to ensure the function name was returned\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "# create a UserProxyAgent ins"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69a81cb9ef79cdff"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dca301a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    # MS updated to search for BAZINGA! to terminate\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\")\n",
    "    and \"BAZINGA!\" in x.get(\"content\", \"\"),  # and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=4,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b9526e7",
   "metadata": {},
   "source": [
    "## Making Function Calls\n",
    "\n",
    "In this example, we demonstrate function call execution with `NexusFunctionCallingAssistant` and `UserProxyAgent`. We allow the LLM assistant to perform tasks with code, and the `UserProxyAgent` would extract code blocks from the LLM response and execute them. With the new \"function_call\" feature, we define functions and specify the description of the function in the OpenAI config for the `NexusFunctionCallingAssistant`. Then we register the functions in `UserProxyAgent`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fb85afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions according to the function description\n",
    "\n",
    "# one way of registering functions is to use the register_for_llm and register_for_execution decorators\n",
    "CurrencySymbol = Literal[\"USD\", \"EUR\"]\n",
    "\n",
    "\n",
    "def exchange_rate(base_currency: CurrencySymbol, quote_currency: CurrencySymbol) -> float:\n",
    "    if base_currency == quote_currency:\n",
    "        return 1.0\n",
    "    elif base_currency == \"USD\" and quote_currency == \"EUR\":\n",
    "        return 1 / 1.1\n",
    "    elif base_currency == \"EUR\" and quote_currency == \"USD\":\n",
    "        return 1.1\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown currencies {base_currency}, {quote_currency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@user_proxy.register_for_execution()\n",
    "@chatbot.register_for_llm(description=\"Currency exchange calculator.\")\n",
    "def currency_calculator(\n",
    "    base_amount: Annotated[float, \"Amount of currency in base_currency\"],\n",
    "    base_currency: Annotated[CurrencySymbol, \"Base currency\"] = \"USD\",\n",
    "    quote_currency: Annotated[CurrencySymbol, \"Quote currency\"] = \"EUR\",\n",
    ") -> str:\n",
    "    quote_amount = exchange_rate(base_currency, quote_currency) * base_amount\n",
    "    return f\"{format(quote_amount, '.2f')} {quote_currency}\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc84164e00e164ad"
  },
  {
   "cell_type": "markdown",
   "id": "f6952220",
   "metadata": {},
   "source": [
    "Finally, we initialize the chat that would use the functions defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d3e43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = user_proxy.initiate_chat(\n",
    "    chatbot,\n",
    "    message=\"How much is 123.45 EUR in USD?\",\n",
    "    summary_method=\"reflection_with_llm\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flaml_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
